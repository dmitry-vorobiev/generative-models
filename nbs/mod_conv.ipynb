{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from typing import Any, Callable, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.stylegan2.layers import EqualConv2d, EqualLinear, EqualLeakyReLU, \\\n",
    "    ModulatedConv2d, AddRandomNoise, ConcatMiniBatchStddev\n",
    "\n",
    "from models.stylegan2.net import StyledLayer, ToRGB, FromRGB, Input, SynthesisNet, MappingNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, mapping: MappingNet, synthesis: SynthesisNet, \n",
    "                 p_style_mix=0.9, w_ema_decay=0.995,\n",
    "                 truncation_psi=0.5, truncation_cutoff=None, \n",
    "                 is_training=True):\n",
    "        super(Generator, self).__init__()  \n",
    "        if is_training:\n",
    "            if w_ema_decay >= 1.0 or w_ema_decay <= 0.0:\n",
    "                w_ema_decay = None\n",
    "            if p_style_mix <= 0.0:\n",
    "                p_style_mix = None\n",
    "            truncation_psi = None\n",
    "        else:\n",
    "            w_ema_decay = None\n",
    "            p_style_mix = None\n",
    "            if truncation_psi >= 1.0:\n",
    "                truncation_psi = None\n",
    "        \n",
    "        self.mapping = mapping\n",
    "        self.synthesis = synthesis\n",
    "        self.p_style_mix = p_style_mix\n",
    "        \n",
    "        self.w_ema_decay = w_ema_decay\n",
    "        self.register_buffer('w_avg', torch.zeros(mapping.style_dim))\n",
    "        \n",
    "        self.truncation_psi = truncation_psi\n",
    "        self.truncation_cutoff = truncation_cutoff\n",
    "\n",
    "    @property\n",
    "    def num_layers(self):\n",
    "        return self.synthesis.num_layers\n",
    "    \n",
    "    def w_ema_step(self, w: Tensor):\n",
    "        with torch.no_grad():\n",
    "            self.w_avg = torch.lerp(w.mean(0), self.w_avg, self.w_ema_decay)\n",
    "        return w\n",
    "\n",
    "    def mix_styles(self, z1: Tensor, label: Tensor, w1: Tensor):\n",
    "        num_layers = self.num_layers\n",
    "        \n",
    "        if random.uniform(0, 1) < self.p_style_mix:\n",
    "            mix_cutoff = int(random.uniform(1, num_layers))\n",
    "        else:\n",
    "            mix_cutoff = num_layers\n",
    "\n",
    "        z2 = torch.randn_like(z1)\n",
    "        w2 = self.mapping(z2, label)\n",
    "        mask = (torch.arange(num_layers) < mix_cutoff)[:, None, None]\n",
    "        return torch.where(mask, w1, w2)\n",
    "    \n",
    "    def truncate(self, w: Tensor):\n",
    "        assert w.ndim == 3, \"w: layer axis is missing\"\n",
    "        layer_psi = torch.ones(self.num_layers, device=w.device)\n",
    "        if self.truncation_cutoff is None:\n",
    "            layer_psi *= self.truncation_psi\n",
    "        else:\n",
    "            layer_idx = torch.arange(self.num_layers, device=w.device)\n",
    "            mask = layer_idx < self.truncation_cutoff\n",
    "            layer_psi = torch.where(mask, layer_psi * self.truncation_psi, layer_psi)\n",
    "        w = torch.lerp(self.w_avg[None, None, :], w, layer_psi[:, None, None])\n",
    "        return w\n",
    "\n",
    "    def forward(self, z, label=None):\n",
    "        w = self.mapping(z, label)\n",
    "        if self.w_ema_decay:\n",
    "            self.w_ema_step(w)\n",
    "        # N, S -> L, N, S\n",
    "        w = w.expand(self.num_layers, -1, -1)\n",
    "        if self.p_style_mix:\n",
    "            w = self.mix_styles(z, label, w)\n",
    "        if self.truncation_psi:\n",
    "            w = self.truncate(w)\n",
    "        return self.synthesis(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 16\n",
    "style_dim = 16\n",
    "\n",
    "synthesis = SynthesisNet(\n",
    "    img_res=32,\n",
    "    fmap_base=2 << 5,\n",
    "    style_dim=style_dim\n",
    ")\n",
    "\n",
    "mapping = MappingNet(\n",
    "    latent_dim=latent_dim,\n",
    "    label_dim=1,\n",
    "    style_dim=style_dim,\n",
    "    num_layers=3,\n",
    "    hidden_dim=16\n",
    ")\n",
    "\n",
    "g = Generator(mapping, synthesis, \n",
    "    p_style_mix=0.9, \n",
    "    truncation_psi=0.5, \n",
    "    truncation_cutoff=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 32, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 3\n",
    "z = torch.rand(N, latent_dim)\n",
    "y = torch.rand(N, 1)\n",
    "g(z, y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FromRGB(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FromRGB, self).__init__()\n",
    "        self.conv = EqualConv2d(in_channels, out_channels, kernel_size=1,\n",
    "                                stride=1, padding=0, bias=True)\n",
    "        self.act_fn = EqualLeakyReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act_fn(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_lrelu(in_ch: int, out_ch: int):\n",
    "    return [EqualConv2d(in_ch, out_ch, kernel_size=3, stride=1, \n",
    "                        padding=1, bias=True),\n",
    "            EqualLeakyReLU(inplace=True)]\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            *conv_lrelu(in_channels, in_channels),\n",
    "            *conv_lrelu(in_channels, out_channels),\n",
    "            nn.AvgPool2d(2))\n",
    "        self.down = nn.Sequential(\n",
    "            EqualConv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.AvgPool2d(2))\n",
    "        \n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.conv(x) + self.down(x)\n",
    "        return x * (1 / math.sqrt(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, fn: Callable[[Any], Tensor]):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x: Tensor):\n",
    "        return self.fn(x)\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x: Tensor):\n",
    "        return x.flatten(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(3,5,2,2).flatten(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_res=1024, img_channels=3, label_dim=0,\n",
    "                 fmap_base=16 << 10, fmap_decay=1.0, fmap_min=1, fmap_max=512,\n",
    "                 mbstd_group_size=4, mbstd_num_features=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        if img_res <= 4:\n",
    "            raise AttributeError(\"Image resolution must be greater than 4\")\n",
    "\n",
    "        res_log2 = int(math.log2(img_res))\n",
    "        if img_res != 2 ** res_log2:\n",
    "            raise AttributeError(\"Image resolution must be a power of 2\")\n",
    "\n",
    "        def nf(stage):\n",
    "            fmaps = int(fmap_base / (2.0 ** (stage * fmap_decay)))\n",
    "            return np.clip(fmaps, fmap_min, fmap_max)\n",
    "        \n",
    "        inp = FromRGB(img_channels, nf(res_log2-1))\n",
    "        main = [ResidualBlock(nf(res-1), nf(res-2)) \n",
    "                for res in range(res_log2, 2, -1)]\n",
    "        \n",
    "        mbstd_ch = mbstd_num_features * int(mbstd_group_size > 1) \n",
    "        out = [*conv_lrelu(nf(1) + mbstd_ch, nf(1)),\n",
    "               Lambda(lambda x: x.flatten(1)),\n",
    "               EqualLinear(nf(1) * 4**2, nf(0), bias=True), \n",
    "               EqualLeakyReLU(inplace=True), \n",
    "               EqualLinear(nf(0), max(label_dim, 1), bias=True)]\n",
    "        if mbstd_ch:\n",
    "            mbstd = ConcatMiniBatchStddev(mbstd_group_size, mbstd_num_features)\n",
    "            out = [mbstd] + out\n",
    "        \n",
    "        self.layers = nn.Sequential(inp, *main, *out)\n",
    "        \n",
    "    def forward(self, image: Tensor, label: Optional[Tensor] = None):\n",
    "        x = self.layers(image)\n",
    "        if label is not None:\n",
    "            x = torch.sum(x * label, dim=1, keepdim=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Discriminator(img_res=64, fmap_base=2 << 6, label_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d(torch.rand(2, 3, 64, 64), torch.rand(2, 3)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:latest] *",
   "language": "python",
   "name": "conda-env-latest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
