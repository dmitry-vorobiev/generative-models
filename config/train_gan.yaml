hydra:
  run:
    dir: /media/dmitry/data/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job_logging:
    handlers:
      file:
        filename: ${hydra.job.name}.log

defaults:
  - dataset: simple

general:
  gpu: 1
  seed: 333

distributed:
  backend: nccl
  url: env://

data:
  root: /media/dmitry/data/ffhq-dataset/thumbnails128x128
  extensions:
    - png
  loader:
    # batch size per each device
    batch_size: 32
    workers: 2

model:
  G:
    class: models.stylegan2.net.Generator
    params:
      img_res: 128
      num_classes: 0
      latent_dim: 128
      fmap_base: 2048
      num_mapping_layers: 4
      mapping_hidden_dim: 128
      p_style_mix: 0.9
      truncation_psi: 0.5
      truncation_cutoff: null
      impl: torch
  D:
    class: models.stylegan2.net.Discriminator
    params:
      img_res: 128
      num_classes: 0
      fmap_base: 2048
      impl: torch

loss:
  G:
    class: models.stylegan2.loss.G_LogisticNSLoss_PathLenReg
    params:
      pl_reg_freq: 4
  D:
    class: models.stylegan2.loss.D_LogisticLoss_R1
    params:
      r1_freq: 16
      r1_gamma: 100

optim:
  G:
    class: torch.optim.Adam
    params:
      lr: 0.002
      betas: [0.0, 0.99]
      eps: 1e-8
  D:
    class: torch.optim.Adam
    params:
      lr: 0.002
      betas: [0.0, 0.99]
      eps: 1e-8

train:
  epochs: 100
  epoch_length: -1
  G_ema: true
  G_ema_on_cpu: false
  G_ema_decay: 0.99778
  # the effective batch size for gradient accumulation
  batch_size: 32

checkpoints:
  load: null
  save_dir: null
  interval_epoch: 1
  interval_iteration: 500
  max_checkpoints: 100

logging:
  model: false
  iter_freq: 100
  stats:
    - G_fake
    - G_loss
    - G_pl
    - D_real
    - D_fake
    - D_loss
    - D_r1

snapshots:
  enabled: true
  save_dir: null
  num_images: 32
  interval_iteration: 100
